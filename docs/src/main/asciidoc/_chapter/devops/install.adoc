[[devops-env-install]]
=== 测试、生产环境安装配置

[IMPORTANT]
====
本文会给出使用代理与不使用代理的安装、配置方式，但强烈推荐使用代理方式，详见 <<proxies>> 。
====

[NOTE]
====
本文各服务使用 ``x.dew.ms`` 做为访问的域名，可根据实际情况替换。
====

==== 服务规划

TIP: 常规的项目研发多会分 ``开发(dev)、测试(test)、预发(pre-prod)/仿真(uat)、生产(prod)`` 等多个环境。

服务整体上三类：

. 公共支撑服务，如 Gitlab 、 Harbor 等，这些服务要求所有环境共用、互通
. 每个环境独立部署的支撑服务， 如 RabbitMQ、PostgreSql、Redis、dnsmasq、Minio 等，出于数据、资源隔离的要求这些服务各个环境分别部署
. 每个环境的Docker与Kubernetes集群，原则上各环境使用独立的集群，如果共用集群时需要使用``namespace``加以区分

.推荐的服务列表
|===
|分类                 | 域名/主机名              | 服务             | 备注

| 公共支撑服务         | domain:gitlab.dew.ms   | Gitlab           | Gitlab及其CI/CD服务
| 公共支撑服务         | domain:harbor.dew.ms   | Harbor           | Docker私有库服务
| 公共支撑服务         | domain:maven.dew.ms    | Maven            | Maven私有库服务
| 环境相关的支撑服务    | /                      | Dnsmasq          | 轻量级DNS解析服务
| 环境相关的支撑服务    | domain:minio.dew.ms    | Minio            | 分布式对象存储服务，用于CI/CD时做为分布式缓存
| 环境相关的支撑服务    | domain:nfs.dew.ms      | NFS              | 用于Kuernetes persistent volumes的文件存储服务，生产环境建议使用云厂商的分布式文件存储或 CephFS，支持的类型见
                                                                   https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes
| 容器集群            | hostname:k8s-X         | Docker           | Docker容器服务，部署到所有Kubernetes所在的节点
| 容器集群            | hostname:k8s-X         | <CNI>            | Kubernetes CNI服务，部署到所有Kubernetes所在的节点，本文使用 Flannel
| 容器集群            | hostname:k8s-masterX   | Kubernetes Master| Kubernetes Master服务，可做HA
| 容器集群            | hostname:k8s-masterX   | Helm tiller      | Helm服务，部署到Kubernetes Master所在节点
| 容器集群            | hostname:k8s-nodeX     | Kubernetes Node  | Kubernetes Node服务，至少3个节点
|===

[IMPORTANT]
====
* 容器集群各节点主机名与IP的映射配置到 /etc/hosts 中
* kubernetes Node 应至少分两个组: ``group=app`` 用于运行应用， ``group=devops`` 用于运行运维管理工具。
====

TIP: 各支撑服务（中间件）的安装见  <<middleware>> ，下文介绍容器服务的安装配置。

==== 基础配置

*以 Centos7 为例，各节点做好ssh免密互访、关闭防火墙、关闭swap、禁用SELINUX*

[source,bash]
----
# 关闭防火墙
systemctl stop firewalld.service
systemctl disable firewalld.service
# 关闭swap
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
# 禁用SELINUX
sed -i s/^SELINUX=.*$/SELINUX=disabled/ /etc/selinux/config
# 创建key
ssh-keygen -t rsa
----

[source,bash]
----
# 每个节点执行完上述命令后再执行ssh复制，每个节点都要执行N次（N为节点数-1）
ssh-copy-id -i ~/.ssh/id_rsa.pub root@k8s-X
----

==== Docker

TIP: https://kubernetes.io/docs/setup/cri/#docker

.安装配置
----
yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2

yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum update -y && yum install -y docker-ce-18.06.2.ce

mkdir /etc/docker

cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF


# 添加代理（可选）
mkdir -p /etc/systemd/system/docker.service.d/
cat >>/etc/systemd/system/docker.service.d/http-proxy.conf <<EOF
[Service]
Environment="HTTP_PROXY=http://<代理host>:<代理端口>" "HTTPS_PROXY=http://<代理host>:<代理端口>" "NO_PROXY=localhost,127.0.0.1,dew.ms"
EOF

systemctl daemon-reload
systemctl restart docker
systemctl enable docker
----

dew-maven-plugin 需要调用 docker 服务，推荐使用独立的一个Docker节点，暴露 DockerD 服务。

.暴露DOCKER_HOST
----
# 详见 https://docs.docker.com/config/daemon/

mkdir -p /etc/systemd/system/docker.service.d/
vi /etc/systemd/system/docker.service.d/docker.conf
-
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd
-
vi /etc/docker/daemon.json
-
{
  "hosts":[
    "unix:///var/run/docker.sock",
    "tcp://0.0.0.0:2375"
  ]
}
systemctl daemon-reload
systemctl restart docker
-
----

==== Kubernetes

TIP: https://kubernetes.io/docs/setup/independent/install-kubeadm/

.安装配置
----
# 使用阿里云镜像加速下载
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl enable --now kubelet

# 查看安装的Kubernetes版本
yum list installed | grep kube
----

TIP: https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

.Master安装配置
----
# 安装Git，后续会用到
yum install -y git

# 初始化Kubernetes，二选一，使用代理方式
kubeadm init \
    --pod-network-cidr=10.244.0.0/16

# 初始化Kubernetes，二选一，不使用代理方式，通过image-repository 及 --kubernetes-version 避免被墙，注意版本与yum安装的版本对应
kubeadm init \
    --image-repository registry.aliyuncs.com/google_containers \
    --kubernetes-version v1.14.1 \
    --pod-network-cidr=10.244.0.0/16

# 记录上述操作输出中的kubeadm join
# e.g.
# kubeadm join 10.200.10.10:6443 --token i3i7qw.2gst6kayu1e8ezlg --discovery-token-ca-cert-hash sha256:cabc90823a8e0bcf6e3bf719abc569a47c186f6cfd0e156ed5a3cd5a8d85fab0

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 查看集群状态
kubectl get cs

# 安装flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml

# 都为Running后表示完成
kubectl get pods --all-namespaces

# 创建命名空间，方便后文使用
kubectl create ns devops
----

[NOTE]
.Master做为Node
====
默认情况下 master 不会做为 node 节点，可通过此命令强制启用（不推荐）

``kubectl taint nodes --all node-role.kubernetes.io/master-``
====

TIP: https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

.Node安装配置
----
# 执行上一步输出的 kubeadm join ...

# 完成后在master上执行情况如下（以1.14.1版本为例）
kubectl get no
-
k8s-master1   Ready    master   22m     v1.14.1
k8s-node1     Ready    <none>   11m     v1.14.1
k8s-node2     Ready    <none>   8m54s   v1.14.1
k8s-node3     Ready    <none>   8m51s   v1.14.1
k8s-node4     Ready    <none>   8m49s   v1.14.1
-
----

.Master HA配置
----
# @see https://kubernetes.io/docs/setup/independent/high-availability/
----

.Node功能划分（打label）
----
kubectl label nodes k8s-nodeX k8s-nodeX ...  group=app
kubectl label nodes k8s-nodeX k8s-nodeX ...  group=devops
----

.添加外部DNS服务，如dnsmasq
----
# 编辑Kubernetes的DNS，加上dew.ms的代理
kubectl -n kube-system edit cm coredns
-
data:
  Corefile: |
    ...
    dew.ms:53 {
        errors
        cache 30
        proxy . x.x.x.x
    }
-
----

==== Helm

TIP: https://docs.helm.sh/using_helm/#installing-helm

.安装配置
----
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF

# 初始化服务，二选一，使用代理方式
helm init --service-account tiller

# 初始化服务，二选一，不使用代理方式，需要指定镜像，注意tiller版本和helm版本对应
helm init --service-account tiller -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1
# 或者初始化之后更换镜像
kubectl set image deployment/tiller-deploy tiller=registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 -n kube-system

# 查看helm版本
helm version

kubectl get pod -n kube-system -l app=helm
----

==== Nginx Ingress Controller

.安装配置
----
# 使用如下方式将80 443暴露出来
helm install stable/nginx-ingress --name dew-nginx --namespace ingress-nginx --version=1.4.0 \
    --set controller.kind=DaemonSet \
    --set controller.hostNetwork=true \
    --set controller.stats.enabled=true \
    --set controller.metrics.enabled=true \
    --set nodeSelector.group=devops
----

==== Dashboard

.安装配置
----
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque
EOF

# 安装，不使用代理方式需要加上 --set image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64
helm install stable/kubernetes-dashboard --name dew-dashboard --namespace kube-system --version=1.4.0 \
    --set rbac.clusterAdminRole=true \
    --set serviceAccount.create=true \
    --set ingress.enabled=true \
    --set-string ingress.annotations."nginx\.ingress\.kubernetes\.io/backend-protocol"="HTTPS" \
    --set ingress.hosts={dashboard.dew.ms} \
    --set ingress.tls[0].hosts={dashboard.dew.ms},ingress.tls[0].secretName=kubernetes-dashboard-certs \
    --set nodeSelector.group=devops

# 获取Token
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep dew-dashboard-kubernetes-dashboard | awk '{print $1}')

# 添加域名到客户机hosts并访问 https://dashboard.dew.ms
----

==== Elasticsearch

TIP: https://github.com/elastic/helm-charts/blob/master/elasticsearch 注意仔细查看各参数设值的说明。

.安装配置
----
app=dew-elasticsearch
size=200Gi

# 创建PV

# 根据replicas的个数来决定下面PV的创建个数
# 在NFS节点中创建NFS目录
for i in {0..1}; do
mkdir -p /data/nfs/elasticsearch/${i}
chmod 775 /data/nfs/elasticsearch/${i}
done

# 在Kubernetes Master节点中创建PV
for i in {0..1}; do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: ${app}
  name: ${app}-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/elasticsearch/${i}
    server: nfs.dew.ms
EOF
done

# 使用helm安装
helm repo add elastic https://helm.elastic.co

helm install --name dew-elasticsearch elastic/elasticsearch --namespace devops --version=6.5.0 \
    --set imageTag=6.6.1 \
    --set clusterName=dew-elasticsearch \
    --set nodeGroup=master \
    --set masterService=dew-elasticsearch-master \
    --set replicas=2 \
    --set minimumMasterNodes=2 \
    --set volumeClaimTemplate.storageClassName="" \
    --set volumeClaimTemplate.resources.requests.storage=200Gi \
    --set fsGroup=0 \
    --set clusterHealthCheckParams="" \
    --set ingress.enabled=true \
    --set ingress.hosts={es.dew.ms} \
    --set nodeSelector.group=devops
----

TIP: 其他Elasticsearch的helm chart : https://github.com/helm/charts/tree/master/stable/elasticsearch

==== Kibana

TIP: https://github.com/helm/charts/tree/master/stable/kibana

.安装配置
----
# 创建PV & PVC

# 在NFS节点中创建NFS目录
mkdir -p /data/nfs/kibana

# 在Kubernetes Master节点中创建PV & PVC
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: kibana
  name: kibana
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/kibana
    server: nfs.dew.ms
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: kibana
  name: kibana
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
      app: kibana
EOF

helm install --name dew-kibana stable/kibana --namespace devops --version=2.2.0 \
    --set image.tag="6.6.1" \
    --set env."ELASTICSEARCH_URL"="http://dew-elasticsearch-master:9200" \
    --set service.internalPort=5601 \
    --set ingress.enabled=true,ingress.hosts={kibana.dew.ms} \
    --set-string ingress.annotations."kubernetes\.io/ingress\.class"=nginx \
    --set-string ingress.annotations."kubernetes\.io/tls-acme"="true" \
    --set ingress.tls[0].hosts={kibana.dew.ms},ingress.tls[0].secretName=kibana-certs \
    --set dashboardImport.enabled=true \
    --set dashboardImport.dashboards."k8s"="https://raw.githubusercontent.com/monotek/kibana-dashboards/master/k8s-fluentd-elasticsearch.json" \
    --set serviceAccount.create=true,serviceAccountName=kibana \
    --set plugins.enabled=true \
    --set persistentVolumeClaim.enabled=true \
    --set persistentVolumeClaim.existingClaim=true \
    --set securityContext.enabled=true \
    --set securityContext.allowPrivilegeEscalation=true \
    --set securityContext.runAsUser=0 \
    --set securityContext.fsGroup=0 \
    --set nodeSelector.group=devops
----

==== Prometheus && Grafana

TIP: https://github.com/helm/charts/tree/master/stable/prometheus-operator

.prometheus-operator结构
----
 |--- prometheus-operator
 |--- prometheus
 |--- alertmanager
 |--- node-exporter
 |--- kube-state-metrics
 |--- service monitors to scrape internal kubernetes components
 |     |---kube-apiserver
 |     |---kube-scheduler
 |     |---kube-controller-manager
 |     |---etcd
 |     |---kube-dns/coredns
 |--- grafana
----

.创建prometheus 的 PV
----
app=prometheus
components=("alertmanager" "prometheus")
size=100Gi

# 在NFS节点中创建NFS目录
for i in ${components[@]};do
mkdir -p /data/nfs/${app}/${i}
done

# 在Kubernetes Master节点中创建PV
for i in ${components[@]};do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    component: ${i}
  name: ${app}-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/${app}/${i}
    server: nfs.dew.ms
EOF
done
----

.创建grafana 的 PV & PVC
----
# 在NFS节点中创建NFS目录
mkdir -p /data/nfs/grafana

# 在Kubernetes Master节点中创建PV & PVC
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: grafana
  name: grafana
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/grafana
    server: nfs.dew.ms
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: grafana
  name: grafana
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  selector:
    matchLabels:
      app: grafana
EOF
----

.使用helm安装
----
# 若需要对etcd进行监控，则需要先创建secret
kubectl -n devops create secret generic dew-prometheus-operator-etcd \
  --from-file=/etc/kubernetes/pki/etcd/ca.crt \
  --from-file=/etc/kubernetes/pki/etcd/peer.crt \
  --from-file=/etc/kubernetes/pki/etcd/peer.key

# 安装，不使用代理要加上 --set kube-state-metrics.image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-state-metrics
# 若要启用对etcd监控，需设置kubeEtcd相关参数。
# grafana.'grafana\.ini'为Grafana的配置参数,请安装时自行修改。
helm install stable/prometheus-operator --name dew-prometheus-operator --namespace devops --version=5.0.10 \
    --set kubelet.serviceMonitor.https=true \
    --set prometheus.ingress.enabled=true \
    --set prometheus.ingress.hosts={prometheus.dew.ms} \
    --set alertmanager.ingress.enabled=true \
    --set alertmanager.ingress.hosts={alertmanager.dew.ms} \
    --set prometheusOperator.securityContext.runAsNonRoot=false \
    --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
    --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
    --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.selector.matchLabels."component"="alertmanager" \
    --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.selector.matchLabels."component"="prometheus" \
    --set prometheus.prometheusSpec.secrets[0]=dew-prometheus-operator-etcd \
    --set kubeEtcd.serviceMonitor.scheme=https \
    --set kubeEtcd.serviceMonitor.insecureSkipVerify=true \
    --set kubeEtcd.serviceMonitor.caFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/ca.crt" \
    --set kubeEtcd.serviceMonitor.certFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/peer.crt" \
    --set kubeEtcd.serviceMonitor.keyFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/peer.key" \
    --set grafana.enabled=true \
    --set grafana.adminPassword=Dew123456 \
    --set grafana.defaultDashboardsEnabled=true \
    --set grafana.ingress.enabled=true \
    --set grafana.ingress.hosts={grafana.dew.ms} \
    --set grafana.ingress.tls[0].host={grafana.dew.ms},ingress.tls[0].secretName=dew-grafana \
    --set grafana.sidecar.dashboards.enabled=true \
    --set grafana.sidecar.dashboards.searchNamespace="devops"\
    --set grafana.sidecar.dashboards.label=grafana_dashboard \
    --set grafana.sidecar.datasources.enabled=true \
    --set grafana.sidecar.datasources.searchNamespace="devops" \
    --set grafana.sidecar.datasources.label=grafana_datasource \
    --set grafana.'grafana\.ini'.smtp.enabled="true" \
    --set grafana.'grafana\.ini'.smtp.host="smtp.163.com:25" \
    --set grafana.'grafana\.ini'.smtp.user=XXXXX@163.com \
    --set grafana.'grafana\.ini'.smtp.password=XXXXX \
    --set grafana.'grafana\.ini'.smtp.from_address="XXXXX@163.com" \
    --set grafana.'grafana\.ini'.smtp.skip_verify=true \
    --set grafana.'grafana\.ini'.server.root_url="https://grafana.dew.ms" \
    --set grafana.persistence.enabled=true \
    --set grafana.persistence.existingClaim=grafana \
    --set prometheusOperator.nodeSelector.group=devops \
    --set alertmanager.alertmanagerSpec.nodeSelector.group=devops \
    --set prometheus.prometheusSpec.nodeSelector.group=devops \
    --set kube-state-metrics.nodeSelector.group=devops \
    --set nodeExporter.nodeSelector.group=devops \
    --set grafana.nodeSelector.group=devops


# grafana默认用户名：admin
# 访问 http://prometheus.dew.ms
# 访问 http://alertmanager.dew.ms
# 访问 https://grafana.dew.ms
----

.常见问题

> 如何查看设置的密码

 kubectl get secret --namespace devops dew-prometheus-operator-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

> 如何重置grafana密码

  进入grafana的容器内部后执行 ``grafana-cli admin reset-admin-password passwordvalue``

> 有pod启动失败,报文件权限拒绝相关问题，如 "opening storage failed: create dir: mkdir /prometheus/wal: permission denied"

 很可能和PV的文件目录的权限有关，检查下权限是否一致，设置对应的securityContext进行排查。例：
 ``kubectl edit statefulset prometheus-dew-prometheus-operator-prometheus -n devops``
 设置securityContext为以下内容
 -
  securityContext:
    fsGroup: 0
    runAsNonRoot: false
    runAsUser: 0
 -

> 通过UI查看prometheus的target中，kube-scheduler、kube-controller处于down状态

 因为它们只能在宿主机上通过127.0.0.1访问，可使用以下操作：
 . 如果使用kubeadm启动的集群，初始化时的config.yml里可以加入如下参数
     controllerManagerExtraArgs:
       address: 0.0.0.0
     schedulerExtraArgs:
       address: 0.0.0.0
 . 已经启动后的使用下面命令更改就会滚动更新
     sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-controller-manager.yaml
     sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-scheduler.yaml
   或者全部替换：
     sed -ri '/--address/s#=.+#=0.0.0.0#' /etc/kubernetes/manifests/kube-*
 . 参考文章：
   http://www.servicemesher.com/blog/prometheus-operator-manual/
   https://github.com/coreos/prometheus-operator/blob/master/Documentation/troubleshooting.md


> 如何监控APP

  1.首先需要将项目instrument
    参考文章：https://prometheus.io/docs/instrumenting/clientlibs/
  2.部署项目及创建进行监控的ServiceMonitor
    注意ServiceMonitor的labels要含有Prometheus-operator创建的Prometheus的serviceMonitorSelector的label。
    详细文章：https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md#related-resources

==== Fluentd

TIP: https://github.com/kiwigrid/helm-charts/tree/master/charts/fluentd-elasticsearch +
     https://kiwigrid.github.io/

.安装配置
----
helm repo add kiwigrid https://kiwigrid.github.io

# 安装，不使用代理要加上 --set image.tag=v2.4.0 --set image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/fluentd-elasticsearch
# 若要启用Prometheus进行监控Fluentd，
# 需要先将Fluentd通过设置service暴露出来，然后设置prometheusRule和serviceMonitor。
# 此配置需结合Prometheus-operator使用。
helm install kiwigrid/fluentd-elasticsearch --name dew-fluentd-es --namespace devops --version=2.8.3 \
    --set elasticsearch.host=dew-elasticsearch-master \
    --set elasticsearch.logstash_prefix=logstash \
    --set service.type=ClusterIP \
    --set service.ports[0].name="monitor-agent" \
    --set service.ports[0].port=24231 \
    --set prometheusRule.enabled=true \
    --set prometheusRule.prometheusNamespace=devops \
    --set prometheusRule.labels.app=prometheus-operator \
    --set prometheusRule.labels.release=dew-prometheus-operator \
    --set serviceMonitor.enabled=true \
    --set serviceMonitor.labels.release=dew-prometheus-operator \
    --set nodeSelector.group=devops
----

==== Jaeger

TIP: https://github.com/jaegertracing/jaeger-operator

.安装配置
----
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/crds/jaegertracing_v1_jaeger_crd.yaml
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role_binding.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/operator.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -

# 创建Jaeger实例
cat <<EOF | kubectl apply -n devops -f -
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
spec:
  strategy: production
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://dew-elasticsearch-master:9200
EOF

# Jaeger实例可在不同namespace下创建使用，使用中请注意namespace的问题。
# 使用sidecar的方式部署项目：https://github.com/jaegertracing/jaeger-operator#auto-injection-of-jaeger-agent-sidecars
# 使用daemonset的方式部署项目：https://github.com/jaegertracing/jaeger-operator#agent-as-daemonset

# 添加Host，修改Jaeger实例的Ingress
kubectl patch ingress jaeger-query -n devops -p "spec:
 rules:
   - host: jaeger.dew.ms
     http:
       paths:
         - backend:
             serviceName: jaeger-query
             servicePort: 16686
           path: /"

# Pod的调度
# 目前jaeger-operator暂不支持直接设置，请关注该项目的更新情况。
# 可以自行给需要调度的pod的deployment添加限制条件。
# e.g.
 kubectl patch deploy jaeger-operator jaeger-collector jaeger-query -n devops -p '{"spec": {"template": {"spec": {"nodeSelector": {"group": "devops"}}}}}'
----

.使用
----
# 在Deployment 和 Service 中添加annotations : sidecar.jaegertracing.io/inject: "true" 即可。
# 使用Dew的 devops-maven-plugin 部署会自动添加此注解。

# 手工添加示例如下：
cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    # 添加 Jaeger的注解
    sidecar.jaegertracing.io/inject: "true"
  name: jaeger-demo
spec:
  template:
    metadata:
      labels:
        app: jaeger-demo
        version: v1
    spec:
      containers:
      - name: jaeger-demo
        image: jaegertracing/example-hotrod:1.10
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  annotations:
   # 添加 Jaeger的注解
    sidecar.jaegertracing.io/inject: "true"
  name: jaeger-demo
  labels:
    app: jaeger-demo
spec:
  ports:
   - name: jaeger-demo
     port: 8080
     targetPort: 8080
  selector:
   app: jaeger-demo
EOF
----
